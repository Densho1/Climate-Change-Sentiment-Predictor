{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b0bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "import wordcloud\n",
    "\n",
    "import nltk.downloader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "params = {\"negative\": -1, \"neutral\": 0, \"positive\": 1}\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set shuffle randomstate; will be changed repeatedly\n",
    "DATASHUFFLE_RANDOMSTATE = 0\n",
    "training_score = 0.7\n",
    "total_score = 0\n",
    "# total_score_sq = 0\n",
    "new_training_score = 0\n",
    "\n",
    "average_score = 0\n",
    "# stdev = 0\n",
    "\n",
    "loop = False\n",
    "\n",
    "while loop:\n",
    "    \n",
    "    print(f\"Average score: %f (+%f) (Iter: %d)\" % (average_score, new_training_score, DATASHUFFLE_RANDOMSTATE))\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "    # Import training dataset\n",
    "    df = pd.read_csv('Climate_Sentiments_Twitter.csv')\n",
    "    # df.head()\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "    # Transform `sympathy?` column to `sympathy`\n",
    "    df.rename(columns={\"sympathy?\":\"sympathy\"}, inplace=True)\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "    # Drop unneeded columns\n",
    "    df = df[['text', 'sympathy']]\n",
    "    df = df.sample(frac=1, random_state=DATASHUFFLE_RANDOMSTATE).reset_index(drop=True)\n",
    "    # df.head()\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "    # Check if there are rows with missing values\n",
    "    # df.isna().value_counts()\n",
    "\n",
    "    #Transforming the sympathy column to numerical values using the replacement method\n",
    "    df['sympathy'].replace(params, inplace=True)\n",
    "    # df.head()\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    #Creating the feature(s) and target vectors\n",
    "    features = df.drop(\"sympathy\", axis=1)\n",
    "    target = df[\"sympathy\"]\n",
    "    # features.head()\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    #Tokenize the text data using RegexpTokenizer\n",
    "    textdata = features['text']\n",
    "    n = len(textdata)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenizedtext = []\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    for i in range(n):\n",
    "        #Convert text data to lowercase\n",
    "        lower = textdata.iloc[i].lower()\n",
    "\n",
    "        #Tokenize\n",
    "        wordsarray = tokenizer.tokenize(lower)\n",
    "        tokenizedtext.append(wordsarray)\n",
    "\n",
    "    #print(tokenizedText)\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "    #Remove stop words using nltk\n",
    "    englishstopwords = set(stopwords.words('english'))\n",
    "    englishstopwords.add(\"amp\")\n",
    "    shortertext = []\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    for tweet in tokenizedtext:\n",
    "        shortertweet = []\n",
    "        for word in tweet:\n",
    "            if word not in englishstopwords:\n",
    "                word = word.strip()\n",
    "                if (word.isdigit() == False and len(word) >= 2):\n",
    "                    shortertweet.append(word)\n",
    "        shortertext.append(shortertweet)\n",
    "\n",
    "    #print(shorterText)\n",
    "    print(\".\", end=\"\")\n",
    "        \n",
    "\n",
    "    #Stem using PorterStemmer\n",
    "    porterstemmer = PorterStemmer()\n",
    "    stemmedtext = []\n",
    "    for tweet in shortertext:\n",
    "        stemmedwords = []\n",
    "        for word in tweet:\n",
    "            stemmedwords.append(porterstemmer.stem(word))\n",
    "        convertback = ' '.join(stemmedwords)\n",
    "        stemmedtext.append(convertback)\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "        \n",
    "    #Create a separate dataframe dfcv for later EDA\n",
    "    # dfcv = df.copy()\n",
    "    # dfcv['text'] = stemmedtext\n",
    "\n",
    "    # dfcv.head()\n",
    "\n",
    "    #Vectorizing the text data using TFIDvectorizer for Modelling\n",
    "    tfid = TfidfVectorizer()\n",
    "    vectorizedtfid = tfid.fit_transform(stemmedtext)\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "    #print(vectorizedtfid)\n",
    "    #print(tfid.vocabulary_)\n",
    "\n",
    "    #\n",
    "    # Use \"vectorizedtfid\" for the features matrix\n",
    "    # and \"target\" dataframe for the target vector\n",
    "    #\n",
    "\n",
    "    # Train, get score\n",
    "    x_train, x_test, y_train, y_test = train_test_split(vectorizedtfid, target, test_size=0.30, shuffle=True, random_state=0)\n",
    "    new_training_score = 0;\n",
    "    for s in ['newton-cg','lbfgs','liblinear']:\n",
    "        # print (\"Solver: \", s)\n",
    "        logisticRegr = LogisticRegression(C=1000000,solver=s,max_iter=1000000)\n",
    "        logisticRegr.fit(x_train,y_train)\n",
    "        score = logisticRegr.score(x_test,y_test)\n",
    "        if s == 'lbfgs':\n",
    "            new_training_score = score\n",
    "        # print(\"Accuracy Score:\", score)\n",
    "    \n",
    "    if new_training_score > training_score:\n",
    "        training_score = new_training_score\n",
    "        print(\"\\n===========[HIGH SCORE]===============\")\n",
    "        print(f\"%d: %f\" % (DATASHUFFLE_RANDOMSTATE, training_score))\n",
    "        print(\"======================================\", end=\"\")\n",
    "    \n",
    "    DATASHUFFLE_RANDOMSTATE += 1\n",
    "    total_score += new_training_score\n",
    "    # total_score_sq += new_training_score * new_training_score\n",
    "    \n",
    "    average_score = total_score / DATASHUFFLE_RANDOMSTATE\n",
    "    # stdev = math.sqrt((total_score_sq/DATASHUFFLE_RANDOMSTATE) - math.pow(total_score/DATASHUFFLE_RANDOMSTATE,2))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
